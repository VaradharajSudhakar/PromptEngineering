{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage, AnyMessage\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import MessagesState,StateGraph, START, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load the environment variables\n",
    "OPENAI_API_BASE = os.getenv(\"OPENAI_API_BASE\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL = os.getenv(\"MODEL\")\n",
    "\n",
    "ollamaModel = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessage(content=\"You are a helpfule assistant tasked with performing arithmatic on a set of inputs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply a and b\n",
    "    \n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Custom Tool Multiplying {a} by {b}\")\n",
    "    return a*b\n",
    "\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add a and b\n",
    "    \n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    print(f\"Custom Tool Adding {a} and {b}\")\n",
    "    return a+b\n",
    "\n",
    "def subtract(a: float, b: float) -> float:\n",
    "    \"\"\"Subtract b from a\n",
    "    \n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    print(f\"Custom Tool Subtracting {b} from {a}\")\n",
    "    return a-b\n",
    "\n",
    "def divide(a: float, b: float) -> float:\n",
    "    \"\"\"Divide a by b\n",
    "    \n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    print(f\"Custom Tool Dividing {a} by {b}\")\n",
    "    return a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tools = [add, subtract, multiply, divide]\n",
    "\n",
    "#llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add, subtract, multiply, divide]\n",
    "\n",
    "llm_with_tools = ollamaModel.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Assistant:\n",
    "    def __init__(self, llm_with_tools):\n",
    "        \"\"\"\n",
    "        Initialize the Assistant with llm. \n",
    "        \"\"\"\n",
    "\n",
    "        self.llm_with_tools = llm_with_tools\n",
    "\n",
    "    def __call__(self, state: MessagesState, config):\n",
    "        \"\"\"\n",
    "        Call method to invoke\n",
    "        \"\"\"\n",
    "        # Get the last message\n",
    "        messsages = state['messages']\n",
    "\n",
    "        result = self.llm_with_tools.invoke([system_message]+messsages)\n",
    "        return {\"messages\":result}\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "#Define the nodes\n",
    "builder.add_node(\"assistant\", Assistant(ollamaModel.bind_tools(tools)))\n",
    "builder.add_node(\"tools\",ToolNode(tools))\n",
    "\n",
    "# Define the edges\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "builder.add_edge(\"tools\",\"assistant\")\n",
    "react_graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Add 3 and 4, then multiple by 2, and finally divide by 5\")]\n",
    "messages = react_graph.invoke({\"messages\":messages})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in messages['messages']:\n",
    "    message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
